{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 40
    },
    "colab_type": "code",
    "id": "g4Xv0e6xbOCT",
    "outputId": "dbdab0d2-2fa9-4df4-b35d-60c8e8f0e670",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import keras\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import datetime\n",
    "\n",
    "import cv2\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 62
    },
    "colab_type": "code",
    "id": "P1oAR9MJbOCa",
    "outputId": "ab479e7d-e69e-494d-e694-e633ef863001"
   },
   "outputs": [],
   "source": [
    "## Set local data path\n",
    "dataPath = 'all'\n",
    "localPath = ''\n",
    "print(os.listdir(dataPath))\n",
    "\n",
    "images = np.load(dataPath + '/train_images.npy', encoding=\"bytes\")\n",
    "labels = pd.read_csv(dataPath + '/train_labels.csv')\n",
    "images.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "colab_type": "code",
    "id": "SSgBRB1RbOCf",
    "outputId": "ee35440a-4c71-45fe-c295-facc0e98f00c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nay19x9sbOCi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gDlgLswYbOCl"
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hXnnXSN-mWcA"
   },
   "outputs": [],
   "source": [
    "## Contour filtering\n",
    "def preProcessImage(image, cutoff=127, areaCutoff=14, maxContours=4, fliplr=False):\n",
    "    image = np.uint8(image)\n",
    "    im = np.uint8(image)\n",
    "    red, thresh = cv2.threshold(im, cutoff, 255, 0)\n",
    "    im2, contours, hierarchy= cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    mask = np.zeros(im.shape, np.uint8)\n",
    "    largest_contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    \n",
    "    for ind, contour in enumerate(largest_contours[:maxContours]):\n",
    "        if cv2.contourArea(contour) > areaCutoff:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            mask[y:y+h, x:x+w] = 255\n",
    "        \n",
    "    filteredImage = cv2.bitwise_and(image, image, mask=mask)\n",
    "    #plt.imshow(filteredImage)\n",
    "    #plt.figure()\n",
    "    #plt.imshow(thresh)\n",
    "    #plt.figure()\n",
    "    #plt.imshow(mask)\n",
    "    if fliplr:\n",
    "        return np.fliplr(filteredImage).reshape((image.shape))\n",
    "    return filteredImage.reshape((image.shape))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X6I5YVd5maQ7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GZQWNgDTmaZZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OpOIeKmg_jgj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QKSibSUg_jgn",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "colab_type": "code",
    "id": "VWydYkIebOCp",
    "outputId": "f460aed6-108d-49d2-9679-769081e50005"
   },
   "outputs": [],
   "source": [
    "num = 100\n",
    "image = images[num][1].reshape(100, 100)\n",
    "betterImage = preProcessImage(np.fliplr(image), areaCutoff=30, maxContours=1)\n",
    "plt.imshow(betterImage)\n",
    "plt.figure()\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2410
    },
    "colab_type": "code",
    "id": "d4bS1W-WbOCt",
    "outputId": "f88aa9a4-faa8-4c80-b528-bb537fb3665e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XhOjx-XfbOCy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umRNpXdKbOC1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fUCpaRJibOC4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KXgOylPQbOC7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GxpdpkR8bOC9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PcAqgJoZbODD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46lV6hfRbODJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ARJYc-kfbODN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fk-CN7IObODQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6wkV2wnSbODU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MjbbOPAAbODX"
   },
   "outputs": [],
   "source": [
    "allData = pd.DataFrame(np.array(list(images[:,1]))).assign(label=labels['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_izRWN1QbODc"
   },
   "outputs": [],
   "source": [
    "num_classes = len(labels['Category'].unique())\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(labels['Category'].unique())\n",
    "valueCounts = labels['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FxcIGFUfbODe"
   },
   "outputs": [],
   "source": [
    "epochs = 250\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42oLfXTzbODg"
   },
   "outputs": [],
   "source": [
    "xTrain, xValid = train_test_split(allData, stratify=labels['Category'], test_size=0.15, random_state=12345)\n",
    "trainInds = xTrain.index\n",
    "validInds = xValid.index\n",
    "xTrainRaw = xTrain.drop('label', axis=1).values.reshape((xTrain.shape[0], 100, 100, 1))\n",
    "xValidRaw = xValid.drop('label', axis=1).values.reshape((xValid.shape[0], 100, 100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JpvPdNfkbODi"
   },
   "outputs": [],
   "source": [
    "yTrainString = labels.iloc[trainInds, 1].values\n",
    "yValidString = labels.iloc[validInds, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ztN9WBVfbODm"
   },
   "outputs": [],
   "source": [
    "yTrain = lb.transform(yTrainString)\n",
    "yValid = lb.transform(yValidString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_y3e6Hq2jRkj"
   },
   "outputs": [],
   "source": [
    "trainIndsPerClass = {}\n",
    "validIndsPerClass = {}\n",
    "\n",
    "for i, label in enumerate(lb.classes_):\n",
    "    trainIndsPerClass.update({label:np.where(yTrain[:, i] == 1)[0]})\n",
    "    validIndsPerClass.update({label:np.where(yValid[:, i] == 1)[0]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nJM4dQfLbODr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pool = ThreadPool(multiprocessing.cpu_count())\n",
    "inds = trainIndsPerClass['empty']\n",
    "example = pool.map(preProcessImage, [xTrainRaw[i] for i in range(xTrainRaw.shape[0]) if i in inds])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JOpd9Tl6bODu"
   },
   "outputs": [],
   "source": [
    "pool = ThreadPool(multiprocessing.cpu_count())\n",
    "xTrainUnflipped = pool.map(preProcessImage, [xTrainRaw[i] for i in range(xTrainRaw.shape[0])])\n",
    "xTrainUnflipped = np.array(xTrainUnflipped)\n",
    "xTrainFlipped = pool.map(lambda im: preProcessImage(im, fliplr=True), [xTrainRaw[i] for i in range(xTrainRaw.shape[0])])\n",
    "xTrainFlipped = np.array(xTrainFlipped)\n",
    "\n",
    "xValidUnflipped = pool.map(preProcessImage, [xValidRaw[i] for i in range(xValidRaw.shape[0])])\n",
    "xValidUnflipped = np.array(xValidUnflipped)\n",
    "xValidFlipped = pool.map(lambda im: preProcessImage(im, fliplr=True), [xValidRaw[i] for i in range(xValidRaw.shape[0])])\n",
    "xValidFlipped = np.array(xValidFlipped)\n",
    "\n",
    "xTrain = np.concatenate((xTrainUnflipped, xTrainFlipped), axis=0)\n",
    "xValid = np.concatenate((xValidUnflipped, xValidFlipped), axis=0)\n",
    "\n",
    "yTrain = np.concatenate((yTrain, yTrain), axis=0)\n",
    "yValid = np.concatenate((yValid, yValid), axis=0)\n",
    "\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "colab_type": "code",
    "id": "NvN_wfcAjyRF",
    "outputId": "3cb431b6-de16-487d-9ce2-cd1aa4653f7b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "colab_type": "code",
    "id": "0C8Tj48BbODw",
    "outputId": "f52be367-3d84-4e4f-86ae-252675e1178a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num = 1011\n",
    "plt.imshow(xTrain[num][:,:,0])\n",
    "plt.figure()\n",
    "plt.imshow(xTrainRaw[num][:,:,0])\n",
    "yTrainString[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "colab_type": "code",
    "id": "iSmzpD-rbOD0",
    "outputId": "0410e9d8-8b2b-4edf-f34a-9e29db11e1cd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XGqT8BXSbOD4"
   },
   "outputs": [],
   "source": [
    "## Basic model\n",
    "modelBasic = Sequential()\n",
    "modelBasic.add(Flatten(input_shape=(100, 100, 1)))\n",
    "\n",
    "modelBasic.add(Dense(64, activation='relu'))\n",
    "modelBasic.add(Dropout(0.25))\n",
    "\n",
    "modelBasic.add(Dense(10, activation='relu'))\n",
    "modelBasic.add(Dropout(0.25))\n",
    "\n",
    "modelBasic.add(Dense(num_classes, activation='softmax'))\n",
    "modelBasic.compile(loss = keras.losses.categorical_crossentropy,\n",
    "             optimizer = keras.optimizers.Adadelta(),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FW3f2Wt7bOD8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tpu-36_dbOD-"
   },
   "outputs": [],
   "source": [
    "## General model taken from online\n",
    "model1 = Sequential()\n",
    "model1.add(Conv2D(15, kernel_size=(3, 3), activation='relu', input_shape=(100, 100, 1)))\n",
    "model1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(5, 5)))\n",
    "model1.add(Dropout(0.25))\n",
    "model1.add(Dense(10, activation='relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(num_classes, activation='softmax'))\n",
    "model1.compile(loss = keras.losses.categorical_crossentropy,\n",
    "             optimizer = keras.optimizers.Adadelta(),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xW1ASwNPbOEA"
   },
   "outputs": [],
   "source": [
    "## Multiple conv layers, no dropout layers\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(100, 100, 1)))\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(10, 10)))\n",
    "\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(num_classes, activation='softmax'))\n",
    "model2.compile(loss = keras.losses.categorical_crossentropy,\n",
    "             optimizer = keras.optimizers.Adadelta(),\n",
    "             metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dQOR6smnbOEF"
   },
   "outputs": [],
   "source": [
    "## Multiple conv layers, no dropout layers, batch normalization\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(100, 100, 1)))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(MaxPooling2D(pool_size=(10, 10)))\n",
    "\n",
    "model3.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(num_classes, activation='softmax'))\n",
    "model3.compile(loss = keras.losses.categorical_crossentropy,\n",
    "             optimizer = keras.optimizers.Adadelta(),\n",
    "             metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8u88qng8bOEI"
   },
   "outputs": [],
   "source": [
    "## Multiple conv layers, dropout layers, batch normalization\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(100, 100, 1)))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dropout(0.25))\n",
    "\n",
    "model4.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model4.add(BatchNormalization())\n",
    "model1.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model4.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dropout(0.25))\n",
    "\n",
    "model4.add(MaxPooling2D(pool_size=(10, 10)))\n",
    "\n",
    "model4.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dropout(0.25))\n",
    "\n",
    "model4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(num_classes, activation='softmax'))\n",
    "model4.compile(loss = keras.losses.categorical_crossentropy,\n",
    "             optimizer = keras.optimizers.Adadelta(),\n",
    "             metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hXB8PkfMbOEM"
   },
   "outputs": [],
   "source": [
    "## Multiple conv layers, dropout layers, batch normalization, average pooling\n",
    "\n",
    "model5 = Sequential()\n",
    "model5.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(100, 100, 1)))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Dropout(0.25))\n",
    "\n",
    "model5.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model5.add(BatchNormalization())\n",
    "model1.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model5.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Dropout(0.25))\n",
    "\n",
    "model5.add(AveragePooling2D(pool_size=(10, 10)))\n",
    "\n",
    "model5.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Dropout(0.25))\n",
    "\n",
    "model5.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model5.add(Flatten())\n",
    "model5.add(Dense(num_classes, activation='softmax'))\n",
    "model5.compile(loss = keras.losses.categorical_crossentropy,\n",
    "             optimizer = keras.optimizers.Adadelta(),\n",
    "             metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rte7bZ-bbOEO"
   },
   "outputs": [],
   "source": [
    "## Online example of similar problem (modified slightly. Currently has exploding parameter issue)\n",
    "adam = Adam(lr=1e-4, decay=1e-6)\n",
    "\n",
    "\n",
    "model6 = Sequential()\n",
    "model6.add(Conv2D(128, kernel_size=(15, 15), padding='same', activation='relu', input_shape=(100, 100, 1)))\n",
    "model6.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model6.add(MaxPooling2D(pool_size=(10, 10), strides=(7, 7), padding='same'))\n",
    "model6.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model6.add(Conv2D(256, kernel_size=(5, 5), padding='same'))\n",
    "model6.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model6.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'))\n",
    "model6.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model6.add(Flatten())\n",
    "\n",
    "model6.add(Dense(384, activation='relu'))\n",
    "model6.add(Dropout(0.5))\n",
    "\n",
    "model6.add(Dense(192, activation='relu'))\n",
    "model6.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "\n",
    "model6.add(Dense(num_classes, activation='softmax'))\n",
    "model6.compile(loss = keras.losses.categorical_crossentropy,\n",
    "             optimizer = adam,\n",
    "             metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZXmvxoEysBgB"
   },
   "outputs": [],
   "source": [
    "## Model3 + leaky relu\n",
    "\n",
    "model7 = Sequential()\n",
    "model7.add(Conv2D(32, kernel_size=(3, 3), input_shape=(100, 100, 1)))\n",
    "model7.add(LeakyReLU(alpha=0.1))\n",
    "model7.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model7.add(Conv2D(64, (3, 3)))\n",
    "model7.add(LeakyReLU(alpha=0.1))\n",
    "model7.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model7.add(MaxPooling2D(pool_size=(8, 8), padding='same'))\n",
    "\n",
    "model7.add(Conv2D(64, (3, 3)))\n",
    "model7.add(LeakyReLU(alpha=0.1))\n",
    "model7.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model7.add(Conv2D(64, (3, 3)))\n",
    "model7.add(LeakyReLU(alpha=0.1))\n",
    "model7.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model7.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "model7.add(Flatten())\n",
    "model7.add(Dense(num_classes, activation='softmax'))\n",
    "model7.compile(loss = keras.losses.categorical_crossentropy,\n",
    "             optimizer = keras.optimizers.Adadelta(),\n",
    "             metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P8ju-m9XmPAt"
   },
   "outputs": [],
   "source": [
    "## Model3 + leaky relu -1 conv2d layers + dropout layers between layers + SGD\n",
    "\n",
    "#opt = SGD(lr=0.0001)\n",
    " \n",
    "model8 = Sequential()\n",
    "model8.add(Conv2D(32, kernel_size=(5, 5), input_shape=(100, 100, 1)))\n",
    "model8.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model8.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "#model8.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model8.add(Conv2D(64, (3, 3)))\n",
    "model8.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model8.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model8.add(Dropout(0.3))\n",
    "model8.add(MaxPooling2D(pool_size=(8, 8), padding='same'))\n",
    "\n",
    "model8.add(Conv2D(64, (3, 3)))\n",
    "model8.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model8.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model8.add(Dropout(0.3))\n",
    "model8.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "model8.add(Flatten())\n",
    "#model8.add(Dropout(0.5))\n",
    "\n",
    "model8.add(Dense(512))\n",
    "model8.add(LeakyReLU(alpha=0.1))\n",
    "model8.add(Dropout(0.5))\n",
    "\n",
    "model8.add(Dense(num_classes, activation='softmax'))\n",
    "model8.compile(loss = keras.losses.categorical_crossentropy,\n",
    "               optimizer = 'adam',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ghRu31i4MJQn"
   },
   "outputs": [],
   "source": [
    "## Something from online https://www.iioab.org/articles/IIOABJ_7.S5_337-341.pdf \n",
    "## inspired by the ts-cnn (For best performance include the BN layers. Got to \n",
    "## about 0.6 accuracy on validation), and use 512 for the second to last dense layer\n",
    "## and use MaxPooling not AveragePooling! Maybe try adding another dense layer at the end if you want. tk\n",
    "## Also use 'adam' as optimizer not the object called adam\n",
    "\n",
    "#opt = SGD(lr=0.0001)\n",
    " \n",
    "adam = Adam(lr=1e-4, decay=1e-6)\n",
    "\n",
    "model9 = Sequential()\n",
    "model9.add(Conv2D(96, kernel_size=(15, 15), input_shape=(100, 100, 1)))\n",
    "model9.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model9.add(MaxPooling2D(pool_size=(3, 3), padding='same'))\n",
    "model9.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model9.add(Dropout(0.5))\n",
    "\n",
    "model9.add(Conv2D(192, (15, 15)))\n",
    "model9.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model9.add(MaxPooling2D(pool_size=(3, 3), padding='same'))\n",
    "model9.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model9.add(Dropout(0.5))\n",
    "\n",
    "model9.add(Conv2D(192, (3, 3)))\n",
    "model9.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model9.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model9.add(Flatten())\n",
    "model9.add(Dropout(0.5))\n",
    "\n",
    "model9.add(Dense(512))\n",
    "model9.add(LeakyReLU(alpha=0.1))\n",
    "model9.add(Dropout(0.5))\n",
    "\n",
    "model9.add(Dense(256))\n",
    "model9.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "\n",
    "model9.add(Dense(num_classes, activation='softmax'))\n",
    "model9.compile(loss = keras.losses.categorical_crossentropy,\n",
    "               optimizer = adam,\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VYiVotcR2Exa"
   },
   "outputs": [],
   "source": [
    "## Interesting model\n",
    "adam = Adam(lr=1e-4, decay=1e-6)\n",
    "\n",
    "model10 = Sequential()\n",
    "model10.add(Conv2D(96, kernel_size=(7, 7), input_shape=(100, 100, 1)))\n",
    "model10.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model10.add(MaxPooling2D(pool_size=(3, 3), padding='same'))\n",
    "model10.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model10.add(Dropout(0.5))\n",
    "\n",
    "model10.add(Conv2D(192, (5, 5)))\n",
    "model10.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model10.add(MaxPooling2D(pool_size=(3, 3), padding='same'))\n",
    "model10.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model10.add(Dropout(0.5))\n",
    "\n",
    "model10.add(Conv2D(192, (3, 3)))\n",
    "model10.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model10.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model10.add(Dropout(0.5))\n",
    "\n",
    "model10.add(Conv2D(192, (3, 3)))\n",
    "model10.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "model10.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model10.add(Flatten())\n",
    "model10.add(Dropout(0.5))\n",
    "\n",
    "model10.add(Dense(512))\n",
    "model10.add(LeakyReLU(alpha=0.1))\n",
    "model10.add(Dropout(0.5))\n",
    "\n",
    "model10.add(Dense(256))\n",
    "\n",
    "model10.add(Dense(num_classes, activation='softmax'))\n",
    "model10.compile(loss = keras.losses.categorical_crossentropy,\n",
    "               optimizer = adam,\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AyQu_29EgzRL"
   },
   "outputs": [],
   "source": [
    "## Best model as of yet (200 or so epochs, 300/400 give more overfit but best overall results still,\n",
    "## max_contours=2, cutoff=127)\n",
    "adam = Adam(lr=1e-3, decay=1e-6)\n",
    "\n",
    "modelBest = Sequential()\n",
    "modelBest.add(Conv2D(192, kernel_size=(15, 15), input_shape=(100, 100, 1), padding='same'))\n",
    "modelBest.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "modelBest.add(MaxPooling2D(pool_size=(3, 3), padding='same'))\n",
    "modelBest.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "modelBest.add(Dropout(0.5))\n",
    "\n",
    "modelBest.add(Conv2D(192, (15, 15), padding='same'))\n",
    "modelBest.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "modelBest.add(MaxPooling2D(pool_size=(3, 3), padding='same'))\n",
    "modelBest.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "modelBest.add(Dropout(0.5))\n",
    "\n",
    "modelBest.add(Conv2D(192, (3, 3), padding='same'))\n",
    "modelBest.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "modelBest.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "modelBest.add(Flatten())\n",
    "modelBest.add(Dropout(0.5))\n",
    "\n",
    "modelBest.add(Dense(512))\n",
    "modelBest.add(LeakyReLU(alpha=0.1))\n",
    "modelBest.add(Dropout(0.5))\n",
    "\n",
    "modelBest.add(Dense(256))\n",
    "\n",
    "modelBest.add(Dense(num_classes, activation='softmax'))\n",
    "modelBest.compile(loss = keras.losses.categorical_crossentropy,\n",
    "               optimizer = adam,\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1052
    },
    "colab_type": "code",
    "id": "HeHcUYzLbOEQ",
    "outputId": "7aea1366-48ef-4b96-ec35-5a67524af723",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "load = False\n",
    "modelName = 'modelBest'\n",
    "model = modelBest\n",
    "\n",
    "if load:\n",
    "    model.load_weights('drive/My Drive/Colab Notebooks/' + modelName + 'weights.hdf5')\n",
    "\n",
    "checkpoints = ModelCheckpoint(filepath='drive/My Drive/Colab Notebooks/' + modelName + 'weights.hdf5', verbose=1, save_best_only=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vP8EfgaF-BUx"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=24,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    horizontal_flip=False)\n",
    "\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(xTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PeHTZdqB-HrE"
   },
   "outputs": [],
   "source": [
    "checkpoints = ModelCheckpoint(filepath='drive/My Drive/Colab Notebooks/' + modelName + 'weights.hdf5', verbose=1, save_best_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 11150
    },
    "colab_type": "code",
    "id": "lo_Pi68z84Yc",
    "outputId": "801993c4-52c0-4a6f-eb1b-974d25cb27ce"
   },
   "outputs": [],
   "source": [
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "model.fit_generator(datagen.flow(xTrain, yTrain, batch_size=128),\n",
    "                    steps_per_epoch=len(xTrain) / 128, epochs=1000, \n",
    "                    verbose=1, validation_data=(xValid, yValid),\n",
    "                    callbacks=[checkpoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lurgzF30bOEX"
   },
   "outputs": [],
   "source": [
    "## Without augmentation\n",
    "history = model.fit(xTrain, \n",
    "                    yTrain, \n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs, verbose=1, \n",
    "                    validation_data=(xValid, yValid),\n",
    "                    callbacks=[checkpoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 62
    },
    "colab_type": "code",
    "id": "gFCbnETsbOEc",
    "outputId": "1643c4b9-ba89-42d6-957d-eb09528582fb"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(xValid, yValid, verbose=0)\n",
    "print('Validation loss:', score[0])\n",
    "print('Validation accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-N4f_0U0bOEg"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves',fontsize=16)\n",
    " \n",
    "# Accuracy Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['acc'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_acc'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves',fontsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vBVKk02BbOEl"
   },
   "outputs": [],
   "source": [
    "## Train classifiers for \n",
    "## Paintbrush vs pencil vs screwdriver,\n",
    "## Pool vs mouth\n",
    "## Mustache vs empty vs squiggle\n",
    "## Skateboard vs rifle\n",
    "## Also CHECK PENCILS AND OTHER SMALL CLASSES FOR THE CONTOUR AREA THING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ppon277yil6w"
   },
   "outputs": [],
   "source": [
    "## Paintbrush Pencil screwdriver discriminator\n",
    "adam = Adam(lr=1e-4, decay=1e-6)\n",
    "\n",
    "paintbrushPencilScrewdriver = Sequential()\n",
    "paintbrushPencilScrewdriver.add(Conv2D(192, kernel_size=(7, 7), input_shape=(100, 100, 1)))\n",
    "paintbrushPencilScrewdriver.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "paintbrushPencilScrewdriver.add(MaxPooling2D(pool_size=(3, 3), padding='same'))\n",
    "paintbrushPencilScrewdriver.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "paintbrushPencilScrewdriver.add(Dropout(0.5))\n",
    "\n",
    "paintbrushPencilScrewdriver.add(Conv2D(192, (5, 5)))\n",
    "paintbrushPencilScrewdriver.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "paintbrushPencilScrewdriver.add(MaxPooling2D(pool_size=(3, 3), padding='same'))\n",
    "paintbrushPencilScrewdriver.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "paintbrushPencilScrewdriver.add(Dropout(0.5))\n",
    "\n",
    "paintbrushPencilScrewdriver.add(Conv2D(192, (3, 3)))\n",
    "paintbrushPencilScrewdriver.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "paintbrushPencilScrewdriver.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "paintbrushPencilScrewdriver.add(Flatten())\n",
    "paintbrushPencilScrewdriver.add(Dropout(0.5))\n",
    "\n",
    "paintbrushPencilScrewdriver.add(Dense(512))\n",
    "paintbrushPencilScrewdriver.add(LeakyReLU(alpha=0.1))\n",
    "paintbrushPencilScrewdriver.add(Dropout(0.5))\n",
    "\n",
    "paintbrushPencilScrewdriver.add(Dense(256))\n",
    "\n",
    "paintbrushPencilScrewdriver.add(Dense(num_classes, activation='softmax'))\n",
    "paintbrushPencilScrewdriver.compile(loss = keras.losses.categorical_crossentropy,\n",
    "               optimizer = adam,\n",
    "               metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uXL034Q57LaK"
   },
   "outputs": [],
   "source": [
    "load = False\n",
    "discName = 'paintbrushPencilScrewdriver'\n",
    "\n",
    "if load:\n",
    "    paintbrushPencilScrewdriver.load_weights('drive/My Drive/Colab Notebooks/' + discName + 'weights.hdf5')\n",
    "\n",
    "checkpoints = ModelCheckpoint(filepath='drive/My Drive/Colab Notebooks/' + discName + 'weights.hdf5', verbose=1, save_best_only=True)\n",
    "paintbrushPencilScrewdriver.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y7cj9MHril9-"
   },
   "outputs": [],
   "source": [
    "paintbrushPencilScrewdriver.fit(xTrain[np.concatenate((trainIndsPerClass['pencil'], trainIndsPerClass['paintbrush'], trainIndsPerClass['screwdriver']))], \n",
    "                                yTrain[np.concatenate((trainIndsPerClass['pencil'], trainIndsPerClass['paintbrush'], trainIndsPerClass['screwdriver']))], batch_size=batch_size,\n",
    "                                epochs=epochs, verbose=1, \n",
    "                                validation_data=(xValid[np.concatenate((validIndsPerClass['pencil'], validIndsPerClass['paintbrush'], validIndsPerClass['screwdriver']))], \n",
    "                                                 yValid[np.concatenate((validIndsPerClass['pencil'], validIndsPerClass['paintbrush'], validIndsPerClass['screwdriver']))]),\n",
    "                                callbacks=[checkpoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cwVOyMdsimEF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lLmox1YzimJT"
   },
   "outputs": [],
   "source": [
    "## Paintbrush Pencil screwdriver discriminator\n",
    "adam = Adam(lr=1e-4, decay=1e-6)\n",
    "\n",
    "squiggleMoustacheEmpty = Sequential()\n",
    "squiggleMoustacheEmpty.add(Conv2D(96, kernel_size=(7, 7), input_shape=(100, 100, 1)))\n",
    "squiggleMoustacheEmpty.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "squiggleMoustacheEmpty.add(MaxPooling2D(pool_size=(3, 3), padding='same'))\n",
    "squiggleMoustacheEmpty.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "squiggleMoustacheEmpty.add(Dropout(0.5))\n",
    "\n",
    "squiggleMoustacheEmpty.add(Conv2D(192, (5, 5)))\n",
    "squiggleMoustacheEmpty.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "squiggleMoustacheEmpty.add(MaxPooling2D(pool_size=(3, 3), padding='same'))\n",
    "squiggleMoustacheEmpty.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "squiggleMoustacheEmpty.add(Dropout(0.5))\n",
    "\n",
    "squiggleMoustacheEmpty.add(Conv2D(192, (3, 3)))\n",
    "squiggleMoustacheEmpty.add(BatchNormalization(momentum=0.99, epsilon=0.00001))\n",
    "squiggleMoustacheEmpty.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "squiggleMoustacheEmpty.add(Flatten())\n",
    "squiggleMoustacheEmpty.add(Dropout(0.5))\n",
    "\n",
    "squiggleMoustacheEmpty.add(Dense(512))\n",
    "squiggleMoustacheEmpty.add(LeakyReLU(alpha=0.1))\n",
    "squiggleMoustacheEmpty.add(Dropout(0.5))\n",
    "\n",
    "squiggleMoustacheEmpty.add(Dense(256))\n",
    "\n",
    "squiggleMoustacheEmpty.add(Dense(num_classes, activation='softmax'))\n",
    "squiggleMoustacheEmpty.compile(loss = keras.losses.categorical_crossentropy,\n",
    "               optimizer = adam,\n",
    "               metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1052
    },
    "colab_type": "code",
    "id": "1nZXIctUimHX",
    "outputId": "b6dd8c7c-bc2b-490c-9c95-e11a5c7a195e"
   },
   "outputs": [],
   "source": [
    "load = True\n",
    "discName = 'squiggleMoustacheEmpty'\n",
    "\n",
    "if load:\n",
    "    squiggleMoustacheEmpty.load_weights('drive/My Drive/Colab Notebooks/' + discName + 'weights.hdf5')\n",
    "\n",
    "checkpoints = ModelCheckpoint(filepath='drive/My Drive/Colab Notebooks/' + discName + 'weights.hdf5', verbose=1, save_best_only=True)\n",
    "squiggleMoustacheEmpty.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 150
    },
    "colab_type": "code",
    "id": "QTueTJmEimBO",
    "outputId": "bd74d18c-1164-47f0-9658-ca3e1aaefe33"
   },
   "outputs": [],
   "source": [
    "squiggleMoustacheEmpty.fit(xTrain[np.concatenate((trainIndsPerClass['squiggle'], trainIndsPerClass['moustache'], trainIndsPerClass['empty']))], \n",
    "                           yTrain[np.concatenate((trainIndsPerClass['squiggle'], trainIndsPerClass['moustache'], trainIndsPerClass['empty']))], batch_size=batch_size,\n",
    "                           epochs=1, verbose=1, \n",
    "                           validation_data=(xValid[np.concatenate((validIndsPerClass['squiggle'], validIndsPerClass['moustache'], validIndsPerClass['empty']))],\n",
    "                                            yValid[np.concatenate((validIndsPerClass['squiggle'], validIndsPerClass['moustache'], validIndsPerClass['empty']))]),\n",
    "                                callbacks=[checkpoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-RDlG5uCCneY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hUb4h63BCnbu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ln81imJMCnYn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 40
    },
    "colab_type": "code",
    "id": "Ix-LarDRYizV",
    "outputId": "73bb6152-dfcd-4bd4-cfb3-73568d4bed82"
   },
   "outputs": [],
   "source": [
    "\n",
    "testImages = np.load(dataPath + '/test_images.npy', encoding=\"bytes\")\n",
    "testImages.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "colab_type": "code",
    "id": "rXv1w4OBbOEu",
    "outputId": "bb7da7bd-0ab3-4612-b4ed-a35aeacf1307"
   },
   "outputs": [],
   "source": [
    "num = 5160\n",
    "image = testImages[num][1].reshape(100, 100)\n",
    "betterImage = preProcessImage(np.fliplr(image), maxContours=1)\n",
    "plt.imshow(betterImage)\n",
    "plt.figure()\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HOyZbpO8bOEx"
   },
   "outputs": [],
   "source": [
    "testData = pd.DataFrame(np.array(list(testImages[:,1]))).assign(label='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BmB-PevGbOEz"
   },
   "outputs": [],
   "source": [
    "xTestRaw = testData.drop('label', axis=1).values.reshape((testData.shape[0], 100, 100, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0y8EPio3bOE3"
   },
   "outputs": [],
   "source": [
    "pool = ThreadPool(multiprocessing.cpu_count())\n",
    "xTest = pool.map(preProcessImage, [xTestRaw[i] for i in range(xTestRaw.shape[0])])\n",
    "xTest = np.array(xTest)\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fsB7rO78bOE7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 40
    },
    "colab_type": "code",
    "id": "vGQKl9-PmE8d",
    "outputId": "dae3b367-bf87-4155-ec2f-8431410b8de7"
   },
   "outputs": [],
   "source": [
    "yTestProbs = model.predict(xTest)\n",
    "yPreds = yTestProbs.argmax(axis=-1)\n",
    "print(yPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3XdrXzhBokHN"
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(list(zip(list(range(10000)), [lb.classes_[x] for x in yPreds])), columns=['Id', 'Category'])\n",
    "now=datetime.datetime.now()\n",
    "\n",
    "sub.to_csv(localPath + '/submission' + str(now.day) + str(now.hour) + '.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3714
    },
    "colab_type": "code",
    "id": "Q--eglHfpQ7S",
    "outputId": "4089a82c-3189-4fb6-961e-cb4142da44c9"
   },
   "outputs": [],
   "source": [
    "## Confusion matrix\n",
    "\n",
    "yValidProbs = model.predict(xValid)\n",
    "yValidPreds = yValidProbs.argmax(axis=-1)\n",
    "print(yValidPreds)\n",
    "print(yValid.argmax(axis=-1))\n",
    "confMat = pd.DataFrame((confusion_matrix([lb.classes_[x] for x in yValid.argmax(axis=-1)], [lb.classes_[x] for x in yValidPreds])), index=['true' + cl for cl in lb.classes_], columns=['pred' + cl for cl in lb.classes_])\n",
    "confMat.to_csv(localPath + '/confusion.csv')\n",
    "print(confMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2410
    },
    "colab_type": "code",
    "id": "MMEjZW7YhcBq",
    "outputId": "017b8ae5-3c6e-4a5d-e322-2c29947267e4"
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv(localPath + '/submission2715.csv')\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v9CpVuBPicBI"
   },
   "outputs": [],
   "source": [
    "def predSquig(i, cat):\n",
    "    if cat in ['squiggle', 'moustache', 'empty']:\n",
    "        testProb = squiggleMoustacheEmpty.predict(xTest[i].reshape((1, 100, 100, 1)))\n",
    "        testPreds = testProb.argmax(axis=-1)\n",
    "        newClass = lb.classes_[testPreds]\n",
    "        return newClass[0]\n",
    "    else:\n",
    "        return cat\n",
    "\n",
    "sub['Category'] = pd.Series([predSquig(i, cat) for i, cat in enumerate(sub['Category'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CeFnJ5dVlWf7"
   },
   "outputs": [],
   "source": [
    "now=datetime.datetime.now()\n",
    "\n",
    "sub.to_csv(localPath + '/submission' + str(now.day) + str(now.hour) + '.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Kaggle Project CNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
